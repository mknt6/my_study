{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 動作確認：早稲田大学RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at nlp-waseda/roberta-base-japanese and are newly initialized: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "from pyknp import Juman\n",
    "MODEL_NAME = \"nlp-waseda/roberta-base-japanese\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)       # トークナイザーのロード\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)    # モデルのロード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JUMAN++を使わない場合のトークナイズ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"仲よくして下されば嬉しいです。\"\n",
    "tokenize = tokenizer.tokenize(sentence)\n",
    "tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"おはようございます\"\n",
    "tokenize = tokenizer.tokenize(sentence)\n",
    "tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語の追加\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)       # トークナイザーのロード\n",
    "tokenizer.add_tokens(\"ございます\")\n",
    "sentence = \"おはようございます\"\n",
    "tokenize = tokenizer.tokenize(sentence)\n",
    "print(\"トークナイズ結果：\",tokenize)\n",
    "print(\"エンコード結果：\",tokenizer.encode(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語の追加\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)       # トークナイザーのロード\n",
    "tokenizer.add_tokens(\"いま\")\n",
    "sentence = \"おはようございます\"\n",
    "tokenize = tokenizer.tokenize(sentence)\n",
    "tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複数文のトークナイズ\n",
    "text_list = [\"私は、その男の写真を三葉、見たことがある。\",\n",
    "             \"一葉は、その男の、幼年時代、とでも言うべきであろうか\",\n",
    "             \"十歳前後かと推定される頃の写真であって、その子供が大勢の女のひとに取りかこまれ\",\n",
    "             \"（それは、その子供の姉たち、妹たち、それから、従姉妹いとこたちかと想像される）\",\n",
    "             \"庭園の池のほとりに、荒い縞の袴はかまをはいて立ち\"]\n",
    "\n",
    "encodings = tokenizer(\n",
    "    text_list,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\", \n",
    "    max_length= 128,\n",
    "    truncation = True,\n",
    "    padding=\"max_length\",\n",
    "    ).input_ids\n",
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割文字のテキストファイルを読み込む\n",
    "splittable_characters_list = []\n",
    "split_kanji_text_path =\"../content/corpus/split-corpus/joyo-kanji-split.txt\"\n",
    "with open(split_kanji_text_path,\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        splittable_characters_list.append(line[0])\n",
    "        splittable_characters_list.append(line[1:3])\n",
    "\n",
    "# 単語がもともとあったら削除\n",
    "for sc in splittable_characters_list:\n",
    "    if tokenizer.vocab.get(sc):\n",
    "        del tokenizer.vocab[sc]\n",
    "\n",
    "# もともとある要らない単語をすべて削除してから追加する\n",
    "for sc in splittable_characters_list:\n",
    "    tokenizer.add_tokens(sc)    # 単語を追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))   # 追加したトークンに合わせてサイズを変更"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JUMAN++を利用した場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"私は犬が好きです。\"\n",
    "jumanpp = Juman(command=\"jumanpp_v2\",\n",
    "                jumanpp=True,\n",
    "                option=\"--config=C:\\jumanpp\\libexec\\jumandic.conf\")   # JUMAN++を使う\n",
    "result = jumanpp.analysis(sentence) # 文章を読み込む\n",
    "result = [mrph.midasi for mrph in result.mrph_list()]\n",
    "print(\"分かち書き結果：\",result)\n",
    "result = \" \".join(result)   # 分かち書き\n",
    "encoding = tokenizer(sentence, return_tensors='pt').input_ids \n",
    "\n",
    "print(\"分かち書き後のトークナイズ結果：\",tokenizer.tokenize(result))\n",
    "print(\"エンコード結果：\",encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"おはようございます\"\n",
    "jumanpp = Juman(command=\"jumanpp_v2\",\n",
    "                jumanpp=True,\n",
    "                option=\"--config=C:\\jumanpp\\libexec\\jumandic.conf\")   # JUMAN++を使う\n",
    "result = jumanpp.analysis(sentence) # 文章を読み込む\n",
    "result = [mrph.midasi for mrph in result.mrph_list()]\n",
    "result = \" \".join(result)   # 分かち書き\n",
    "print(result)\n",
    "print(tokenizer.tokenize(result))\n",
    "encoding = tokenizer(result, return_tensors='pt').input_ids \n",
    "encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> トークナイズ結果： ['▁おはよう', 'ございます']  \n",
    "> エンコード結果： [2, 19283, 32000, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トークナイズ後の結果が異なることがわかる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"自分を大七刀にしてくれる\"\n",
    "jumanpp = Juman(command=\"jumanpp_v2\",\n",
    "                jumanpp=True,\n",
    "                option=\"--config=C:\\jumanpp\\libexec\\jumandic.conf\")   # JUMAN++を使う\n",
    "result = jumanpp.analysis(sentence) # 文章を読み込む\n",
    "result = [mrph.midasi for mrph in result.mrph_list()]\n",
    "print(result)\n",
    "result = \" \".join(result)   # 分かち書き\n",
    "print(result)\n",
    "print(tokenizer.tokenize(result))\n",
    "encoding = tokenizer(result, return_tensors='pt').input_ids \n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"紹\"\n",
    "print(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([4835])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentencePieceの仕様上アンダースコアが文字の前に入ることを確かめる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"../content/model/nlp-waseda/roberta-base-japanese/spiece.model\")\n",
    "\n",
    "sp.EncodeAsPieces(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同じにならないのはなぜ？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 東北大BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "MODEL_NAME = \"../content/model/bert-base-japanese-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)       # トークナイザーのロード\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)    # モデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割文字のテキストファイルを読み込む\n",
    "splittable_characters_list = []\n",
    "split_kanji_text_path =\"../content/corpus/split-corpus/joyo-kanji-split.txt\"\n",
    "with open(split_kanji_text_path,\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        splittable_characters_list.append(line[0])\n",
    "        splittable_characters_list.append(line[1:3])\n",
    "\n",
    "# 単語がもともとあったら削除\n",
    "for sc in splittable_characters_list:\n",
    "    if tokenizer.vocab.get(sc):\n",
    "        del tokenizer.vocab[sc]\n",
    "\n",
    "# もともとある要らない単語をすべて削除してから追加する\n",
    "for sc in splittable_characters_list:\n",
    "    tokenizer.add_tokens(sc)    # 単語を追加\n",
    "\n",
    "text = \"自分を大七刀にしてくれる\"\n",
    "encoding = tokenizer.tokenize(text)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['自分', 'を', '大', '七', '刀', 'に', 'し', 'て', 'くれる']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"自分を大七刀にしてくれる\"\n",
    "encoding = tokenizer.tokenize(text)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sns_path = \"../content/data/result/mask-model-tweet.tsv\"\n",
    "save_path = \"../content/data/tmp/sample-tweet-test.tsv\"\n",
    "df = pd.read_table(sns_path)\n",
    "# display(df)\n",
    "df.columns\n",
    "df = df.drop([\" is_correct \",\" correct_score \",\" incorrect_score \"],axis=1)\n",
    "df = df.rename(columns={\"input \":\"input_ids\",\n",
    "                        \" output \":\"labels\",\n",
    "                        \" choices \":\"option\",\n",
    "                        \" answer \":\"answer\"})\n",
    "\n",
    "df.to_csv(save_path,sep=\"\\t\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "with open(\"kakunin_preds.csv\",\"r\") as f:\n",
    "    line = f.read()\n",
    "    l_preds = ast.literal_eval(line)\n",
    "\n",
    "with open(\"kakunin_labels.csv\",\"r\") as f:\n",
    "    line = f.read()\n",
    "    l_labels = ast.literal_eval(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"../content/model/bert-base-japanese-v2/target_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)       # トークナイザーのロード\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)    # モデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(l_preds)):\n",
    "    if l_preds[idx] != l_labels[idx]:\n",
    "        print(\"l_preds:\",tokenizer.decode(l_preds[idx]))\n",
    "        print(\"l_labels:\",tokenizer.decode(l_labels[idx]))\n",
    "        print(\"------\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 女 の 気 才寺 ち 考えれ ない 奴 が 恋愛 す ん な バーカ 。 [SEP]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([    2,  1762,   896,  2868, 32598,   883, 10866,  5856, 10584, 33003,\n",
    "          862, 14608,   875,   933,   892, 11303,  5777,   829,     3, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'奴'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([33003])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "\n",
    "filepath = \"../reports/Book1.xlsx\"\n",
    "book = openpyxl.load_workbook(filepath)\n",
    "ws = book[\"Sheet1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws.append([\"aa\",\"bb\"])\n",
    "book.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "MODEL_NAME = \"../models/bert-base-japanese-v2/sample_model\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)    # モデルのロード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "text_filepath = Path(\"../data/raw/article/CC-100_ja.txt\")\n",
    "count = 0\n",
    "with open(text_filepath,\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        print(line.strip())\n",
    "        count += 1\n",
    "        if count==100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import mojimoji\n",
    "\n",
    "input_filepath = \"../data/raw/article/mainichi_all.txt\"\n",
    "sample_output = \"../data/interim/article/normed_mainichi_all.txt\"\n",
    "output = [] \n",
    "N = 128 # 閾値\n",
    "\n",
    "\"\"\"文章を区切る\"\"\"\n",
    "with open(input_filepath,\"r\",encoding=\"utf-8\") as f,\\\n",
    "    open(sample_output,\"w\",encoding=\"utf-8\") as sf:\n",
    "    for line in f:\n",
    "        # split_sentence = re.findall(\"[^。]+。?\",line)\n",
    "        split_sentences = re.split(\"(?<=。)\",line)\n",
    "        del split_sentences[-1]     # 改行文字を削除\n",
    "        \n",
    "        tmp = None\n",
    "        for sentence in split_sentences:\n",
    "            \"\"\"正規化を書き込む\"\"\"\n",
    "            normed_sentence = mojimoji.zen_to_han(sentence,kana=False)\n",
    "            \n",
    "            if tmp is None:\n",
    "                \"\"\"tmpが初期値の場合\"\"\"\n",
    "                tmp = normed_sentence\n",
    "            elif len(tmp) + len(normed_sentence) >= N:\n",
    "                \"\"\"分割して書き込み、tmpを初期値へ\"\"\"\n",
    "                output.append(tmp)\n",
    "                output.append(normed_sentence)\n",
    "                tmp = None\n",
    "            elif len(tmp) + len(normed_sentence) < N:\n",
    "                \"\"\"短い文は統合\"\"\"\n",
    "                tmp = tmp + normed_sentence\n",
    "        \n",
    "        if tmp is not None:\n",
    "            output.append(tmp)\n",
    "    \n",
    "    # 改行コードを挿入\n",
    "    output = map(lambda x:x+\"\\n\",output)\n",
    "    sf.writelines(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "sample_list=[1,2,3,4]\n",
    "\n",
    "def add1(sample):\n",
    "    sample = sample+1\n",
    "    return sample\n",
    "for sample in sample_list:\n",
    "    a = add1(sample)\n",
    "    print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9664535356921388"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(1234)\n",
    "random.random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|██████████| 174/174 [00:00<00:00, 87.0kB/s]\n",
      "Downloading: 100%|██████████| 230k/230k [00:00<00:00, 467kB/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM,AutoTokenizer\n",
    "\n",
    "model_name = \"cl-tohoku/bert-base-japanese-v2\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 3]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [UNK]:1\n",
    "def read_splitchar(vocab_filepath):\n",
    "    # 分割文字のテキストファイルを読み込む\n",
    "    splittable_characters_list = []\n",
    "    with open(vocab_filepath,\"r\",encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            splittable_characters_list.append(line[0])\n",
    "            \n",
    "    return splittable_characters_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4c9674c63a624f4c2dc90e1acbddef8b078a38c92d1440a45d7c6038b54e872"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
